{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.mlreview.com/topic-modeling-with-scikit-learn-e80d33668730\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc359184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2673782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset.data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29727c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0415f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e10bbd",
   "metadata": {},
   "source": [
    "<h3>Thoughts</h3>\n",
    "\n",
    "LDA is for Topic Modeling. Topic Modeling is for larger/longer documents, and you can assign categories to documents to group them. This is not what I'm trying to do.\n",
    "\n",
    "I need to use Named Entity Recognition (NER). This can take a single stentence and break down the entities within it. This is frequently used for Chatbots.\n",
    "\n",
    "Example: \n",
    "\n",
    "<i>Summer Hirst went to Ravensbourne University in 2010 and met Emily Victor there.</i>\n",
    "\n",
    "NER can recognize Summer Hirst as a person, Ravensbourne University as a university, 2010 as a date, and Emily Victor as another person.\n",
    "\n",
    "This needs intensive labeling to understand separate words and the categories they belong to. Apart from labeling, the model also needs to understand the context to remove ambiguity. Once the ambiguity is removed, it can be used for extracting information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nanonets.com/blog/named-entity-recognition-with-nltk-and-spacy/\n",
    "# Step One: Import nltk and download necessary packages\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    " \n",
    "# Step Two: Load Data\n",
    " \n",
    "sentence = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "\n",
    "# Step Three: Tokenise, find parts of speech and chunk words \n",
    "\n",
    "for sent in nltk.sent_tokenize(sentence):\n",
    "  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "     if hasattr(chunk, 'label'):\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ac527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1c3f0",
   "metadata": {},
   "source": [
    "[Blog](https://blog.hubspot.com/website/python-ai-chat-bot#:~:text=Exploring%20Natural%20Language%20Processing%20(NLP)%20in%20Python&text=The%20ultimate%20objective%20of%20NLP,user%20queries%20in%20human%20language.)\n",
    "\n",
    "Python ChatBot libraries:\n",
    "* ChatterBot\n",
    "* Rasa\n",
    "* DialogFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640da83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/10/complete-guide-to-build-your-ai-chatbot-with-nlp-in-python/\n",
    "import transformers\n",
    "\n",
    "# nlp = transformers.pipeline(\"conversational\", model=\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# input_text = \"hello!\"\n",
    "# nlp(transformers.Conversation(input_text), pad_token_id=50256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd64012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rubikscode.net/2022/04/25/text-summarization-with-huggingface-transformers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ddafa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chatterbot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3747ba2fdfe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chatterbot'"
     ]
    }
   ],
   "source": [
    "import chatterbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63940cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-279c49635b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f272af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-02-29 14:20:04'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "now = datetime.now(pytz.timezone('US/Pacific')).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b25d73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 2, 29, 15, 15, 45, 948278)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dc04d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {\n",
    "    \"event\": \"call mom\",\n",
    "    \"timestamp\": \"2024-03-02 12:00:00 PST\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a4a2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = f\"You'll get a reminder to {event_dict['event']} at {event_dict['timestamp']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c94810ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You'll get a reminder to call mom at 2024-03-02 12:00:00 PST\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09b187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis on 50k imdb movie reviews\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open('/Users/mike/Downloads/aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b8d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]],\n",
    "                          ignore_index=True)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61e14bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['review', 'sentiment']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25aa0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Shuffle data since they are currently sorted\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad6146d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm csv is saved by trying to read it back\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b46e5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4dde5e",
   "metadata": {},
   "source": [
    "<h3> word2vec </h3>\n",
    "\n",
    "A more modern alternative to the bag-of-words model is <b>word2vec</b>, an algorithm that Google released in 2013. The word2vec algorithm is an unsupervised learning algorithm based on neural networks that attempts to automatically learn the relationships between words. The idea behind it is to put words that have similar meanings into similar clusters, and via clever vector-spacing, the model can reproduce certain words using simple vector math, for example, <i> king - man + woman = queen. </i>\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9ede15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "# Transform the text to numerical feature vectors (bag-of-words model)\n",
    "# Construct bag-of-words model based on word counts in the documents using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "oneGramCount = CountVectorizer()\n",
    "twoGramCount = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining, the weather is sweet, and one and one is two'\n",
    "])\n",
    "\n",
    "# transform the three sentences into sparse feature vectors\n",
    "bag = oneGramCount.fit_transform(docs)\n",
    "# The values represent the index at which the word is stored\n",
    "print(oneGramCount.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3c8b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# The first index position here represents the word 'and' (index 0 from vocabulary_ dict). It is 0 except for \n",
    "# the last document, where 'and' appears twice. These values are called raw term frequencies tf(t,d) --\n",
    "# the number of times a term t occurs in a document d\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c56fa4",
   "metadata": {},
   "source": [
    "The sequence of items in the bag-of-words model just created is also called the <b>1-gram</b> or <b>unigram</b> model -- each item or token in the vocabulary represents a single word. The contiguous sequences of items in NLP -- words, letters, or symbols -- are also called <b>n-grams</b>. n-grams of size 3 and 4 yield good performances for anti-spam filtering for email messages. 1-gram vs 2-gram representation for the first document \"the sun is shining\" would be:\n",
    "\n",
    "* 1-gram: \"the\", \"sun\", \"is\", \"shining\"\n",
    "* 2-gram: \"the sun\", \"sun is\", \"is shining\"\n",
    "\n",
    "We can use different n-gram models with `CountVectorizer` (1-gram is the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22b6b0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(oneGramCount.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00575584",
   "metadata": {},
   "source": [
    "As we saw in the previous section, the word 'is' had the largest term frequency in the third document (it appeared 3 times). After transforming the same feature vector into tf-idfs, we see that the word 'is' is now associated with a relatively small tf-idf (0.45) in the third document. This is because it is also present in the first and second documents and thus is unlikely to contain any useful or discriminatory information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8324f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the sun': 9, 'sun is': 7, 'is shining': 1, 'the weather': 10, 'weather is': 11, 'is sweet': 2, 'shining the': 6, 'sweet and': 8, 'and one': 0, 'one and': 4, 'one is': 5, 'is two': 3} \n",
      "\n",
      "[[0.   0.58 0.   0.   0.   0.   0.   0.58 0.   0.58 0.   0.  ]\n",
      " [0.   0.   0.58 0.   0.   0.   0.   0.   0.   0.   0.58 0.58]\n",
      " [0.57 0.22 0.22 0.28 0.28 0.28 0.28 0.22 0.28 0.22 0.22 0.22]]\n"
     ]
    }
   ],
   "source": [
    "print(twoGramCount.vocabulary_, '\\n')\n",
    "print(tfidf.fit_transform(twoGramCount.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138a3636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the text data\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20708efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75730033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39781193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor('</a>This :) is :( a test :-)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e91ab5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean entire df\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f6ff9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize document by splitting each text corpora into individual elements\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9832d48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word stemming by transforming word to its root form, see below\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.lower().split()]\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515753e",
   "metadata": {},
   "source": [
    "The Porter stemming algorithm is one of the oldest and simplest stemming algorithms. There are other stemming algorithms including the <b>Snowball stemmer</b> (Porter2 or English stemmer) and the <b>Lancaster stemmer</b> (Paice/Husk stemmer), which is faster but also more aggressive than the Porter stemmer. These are also available via NLTK.\n",
    "\n",
    "Note that in the above example, stemming can create non-real words, such as 'thu' (from 'thus'). A technique called <b>lemmmatization</b> aims to obtain the canonical (grammatically correct) forms of individual words -- the so called <b>lemmas</b>. However, lemmatization is computationally more difficult and expensive compared to stemming and, in practice, it has been observed that stemming and lemmatization have little impact on the performance of text classification. \n",
    "\n",
    "Another useful topic is <b>stop-word-removal</b>. Stop-words are those that are extremely common in all sorts of texts and probably bear no (or little) useful information that can be used to distinguish between different classes and documents. Examples of stop-words are <i>is, and, has, like</i>. Removing stop-words can be useful if we are working with raw or normalized term frequencies rather than tf-idfs, which are already downweighting frequently occurring words.\n",
    "\n",
    "In order to remove the stop-words from the movie reviews, we will use the set of 179 English stop-words that are available in the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba41ea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mike/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0aeb06be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14434bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'thu', 'run']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in tokenizer_porter('a runner likes running and thus they run') if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc040b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model to classify the movie reviews as positive or negative\n",
    "# Divide df into 25k documents for training and 25k for testing\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "634cb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [\n",
    "    {\n",
    "    'vect__ngram_range': [(1, 1)],\n",
    "    'vect__stop_words': [stop, None],\n",
    "    'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': [1.0, 10.0, 100.0]\n",
    "    },\n",
    "    {\n",
    "    'vect__ngram_range': [(1, 1)],\n",
    "    'vect__stop_words': [stop, None],\n",
    "    'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "    'vect__use_idf': [False],\n",
    "    'vect__norm': [None],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': [1.0, 10.0, 100.0]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ba1fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [ nan  nan  nan  nan 0.89 0.89 0.89 0.89  nan  nan  nan  nan 0.9  0.89\n",
      " 0.9  0.89  nan  nan  nan  nan 0.89 0.88 0.89 0.88  nan  nan  nan  nan\n",
      " 0.88 0.87 0.88 0.88  nan  nan  nan  nan 0.87 0.86 0.88 0.87  nan  nan\n",
      "  nan  nan 0.87 0.86 0.88 0.87]\n",
      "  warnings.warn(\n",
      "/Users/mike/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf',\n",
    "                     LogisticRegression(random_state=0))])\n",
    "# settings n_jobs = -1 uses all cores on machine to speed up grid search\n",
    "# may not work on windows machines\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e2aac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7f9229372e50>} \n"
     ]
    }
   ],
   "source": [
    "You print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca7fab",
   "metadata": {},
   "source": [
    "As we can see from the above output, the best grid search results came from using the regular `tokenizer` without Porter stemming, no stop-word library, and tf-idfs in the combination with logistic regression classifier using L2-regularization with the regularization strength C of 10.0.\n",
    "\n",
    "Let's use the best model from this grid search and print the average 5-fold cross-validation accuracy scores on the training set and the classification accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9442416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.897\n"
     ]
    }
   ],
   "source": [
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac7c49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0cfd84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c2759f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([0]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([\"This movie is awesome\"]), clf.predict([\"This movie sucks!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66ba1f",
   "metadata": {},
   "source": [
    "This shows that we are able to predict whether a movie review is positive or negative with nearly 90% accuracy. Another popular classifier for text classification is the Naïve Bayes classifier, which gained its popularity in applications of email spam filtering. Naïve Bayes classifiers are easy to implement, computationally efficient, and tend to perform particularly well on relatively small datasets compared to other algorithms. Here is an article discussing them on arXiv: https://arxiv.org/pdf/1410.5329v3.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946f529",
   "metadata": {},
   "source": [
    "<h3>Working with bigger data -- online algorithms and out-of-core learning</h3>\n",
    "\n",
    "The code to train the grid search above took 32 minutes to run. It can be computationally quite expensive to construct the feature vectors for the 50,000 movie review dataset during grid search. In many real-world applications, it is not uncommon to work with even larger datasets that can exceed our computer's memory. Since not everyone has access to supercomputer facilities, there's a technique called <b>out-of-core learning</b>, which allows us to work with such large datasets by fitting the classifier incrementally on smaller batches of the dataset. We've already discussed <b>stochastic gradient descent</b>, which is an optimization algorithm that updates the model's weights using one sample at a time. In this section, we will make use of the `partial_fit` function of the `SGDClassifier` in scikit-learn to stream the documents directly from our local drive, and train a logistic regression model using small mini-batches of documents.\n",
    "\n",
    "First, we define a `tokenizer` function that cleans the unprocessed text data from the `movie_data.csv` file that we constrcuted at the beginning of this notebook and separate it into word tokens while removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d8c7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e5d9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    \"\"\"Generator that reads and returns one document at a time\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "075e6458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should return tuple consisting of review text and class label\n",
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0831dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f645a55",
   "metadata": {},
   "source": [
    "We cannot use `CountVectorizer` for out-of-core learning since it requires holding the complete vocabulary in memory. Also, `TfidfVectorizer` needs to keep all the feature vectors of the training dataset in memory to calculate the inverse document frequencies. However, another useful vectorizer for text processing implemented in scikit-learn is `HashingVectorizer` -- it is data-independent and makes use of the hashing trick via the 32-bit MurmurHash3 function https://en.wikipedia.org/wiki/MurmurHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce35aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer\n",
    "                        )\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f167d",
   "metadata": {},
   "source": [
    "The above cell initialized a `HashingVectorizer` with our tokenizer function and set the number of features to `2**21`. We reinitialized a logistic regression classifier by setting the `loss` parameter of the `SGDClassifier` to `log`. Note that by using a large number of features in the `HashingVectorizer`, we reduce the chance of causing hash collisions, but we also increase the number of coefficients in our logistic regression model. Having set up all the complementary functions, we can now start the out-of-core learning using the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69641653",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e93364",
   "metadata": {},
   "source": [
    "Here we iterated over 45 mini-batches of documents where each mini-batch consists of 1,000 documents. Having completed the incremental learning process, we will use the last 5,000 documents to evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd40380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d7989",
   "metadata": {},
   "source": [
    "As we can see, the accuracy of the model is around 87%, only slightly below the accuracy achieved with the grid search for hyperparameter tuning. However, out-of-core learning is extremely memory efficient and took less than a minute to complete. Finally, we can use the last 5,000 documents to update our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14f4a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404237c",
   "metadata": {},
   "source": [
    "<h3>Topic Modeling with Latent Dirichlet Allocation (LDA)</h3>\n",
    "\n",
    "Topic modeling describes the broad task of assigning topics to unlabelled text documents. For example, a typical application would be the categorization of documents in a large corpus of newspaper articles where we don't know on which specific page or category they appear in. \n",
    "\n",
    "In applications of topic modeling, we aim to assign category labels to those articles -- for example, sports, finance, world news, politics, local news, etc. We can consider topic modeling as a clustering task, a subcategory of unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af5e0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: finish out this section, do Flask Web App section first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b96c4",
   "metadata": {},
   "source": [
    "<h3>Serializing fitted scikit-learn estimators</h3>\n",
    "\n",
    "Training a machine learning model can be computationally quite expensive, as we have seen. We definitely do not want to train our model every time we close our Python interpreter and want to make a new prediction or reload our web application. One option for model persistence is Python's built in `pickle` module, which allows us to serialize and deserialize Python object structures to compact bytecode so that we can save our classifier in its current state and reload it if we want to classify new samples, without needing the model to learn from the training data all over again.\n",
    "\n",
    "Before executing the following code, make sure you have trained the out-of-core logistic regression model from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "177ed3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43ca97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(stop,\n",
    "           open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "249fbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf,\n",
    "           open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa858f",
   "metadata": {},
   "source": [
    "Using this code, we created a `movieclassifier` directory where we will later store the files and data for our web application. Within the `movieclassifier` directory, we created a `pkl_objects` subdirectory to save the serialized Python objects to our local drive. Via the `dump` method, we then serialized the trained logistic regression model as well as the stop word set from the NLTK library, so that we don't have to install the NLTK vocabulary on our server.\n",
    "\n",
    "The `dump` method takes as its first argument the object that we want to pickle, and for the second argument we provided an open file object that the Python object will be written to. Via the `wb` argument inside the `open` function, we opened the file in binary mode for pickle, and we set `protocol=4` to choose the latest and most efficient protocol. (It actually goes up to 5 now, but I'll still use 4 here for consistency: https://docs.python.org/3/library/pickle.html).\n",
    "\n",
    "Note that our logistic regression model contains several NumPy arrays, such as the weight vector, and a more efficient way to serialize NumPy arrays is to use the alternative `joblib` library. To ensure compatability with the server environment used later on, we will use the standard pickle approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16e473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
